{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "054b1a86",
   "metadata": {},
   "source": [
    "# Naive Bayes \n",
    "Naive Bayes is a classification algorithm based on Bayes' Theorem with the \"naive\" assumption of conditional independence between every pair of features given the class value.\n",
    "```bash\n",
    "Bayes Theorem Foundation\n",
    "Bayes Theorem:\n",
    "P(A|B) = [P(B|A) × P(A)] / P(B)\n",
    "\n",
    "In classification context:\n",
    "\n",
    "P(Class|Features) = [P(Features|Class) × P(Class)] / P(Features)\n",
    "\n",
    "The \"Naive\" Assumption\n",
    "The algorithm assumes that all features are conditionally independent given the class:\n",
    "\n",
    "P(Feature₁, Feature₂, ..., Featureₙ|Class) = P(Feature₁|Class) × P(Feature₂|Class) × ... × P(Featureₙ|Class)\n",
    "\n",
    "This simplifies the computation dramatically, even though this assumption is rarely true in real life (hence \"naive\").\n",
    "\n",
    "Mathematical Formulation\n",
    "For a given instance with features x₁, x₂, ..., xₙ, we predict the class C that maximizes:\n",
    "\n",
    "P(C|X) ∝ P(C) × Π P(xᵢ|C)\n",
    "\n",
    "Where:\n",
    "\n",
    "P(C) = Prior probability of class C\n",
    "\n",
    "P(xᵢ|C) = Likelihood of feature xᵢ given class C\n",
    "\n",
    "Π = Product over all features\n",
    "\n",
    "Types of Naive Bayes Classifiers\n",
    "1. Gaussian Naive Bayes\n",
    "Assumes continuous features follow normal distribution\n",
    "\n",
    "Used for continuous data\n",
    "\n",
    "2. Multinomial Naive Bayes\n",
    "Used for discrete counts (e.g., word counts in text)\n",
    "\n",
    "Common for text classification\n",
    "\n",
    "3. Bernoulli Naive Bayes\n",
    "Used for binary/boolean features\n",
    "\n",
    "Common for document classification with word presence/absence\n",
    "\n",
    "Step-by-Step Example 1: Weather Classification\n",
    "Problem: Predict whether to play tennis based on weather conditions\n",
    "\n",
    "Training Data:\n",
    "Outlook\tTemperature\tHumidity\tWind\tPlay Tennis\n",
    "Sunny\t Hot\t    High     \tWeak\t      No\n",
    "Sunny\t Hot\t    High\t   Strong\t      No\n",
    "Overcast Hot\t    High\t   Weak\t          Yes\n",
    "Rain\t Mild\t    High\t   Weak\t          Yes\n",
    "Rain\t Cool\t    Normal\t   Weak\t          Yes\n",
    "Rain\t Cool\t    Normal\t   Strong         No\n",
    "Overcast Cool\t    Normal\t   Strong\t      Yes\n",
    "Sunny\t Mild\t    High\t   Weak\t          No\n",
    "Sunny\t Cool\t    Normal\t   Weak\t          Yes\n",
    "Rain\t Mild\t    Normal\t   Weak\t          Yes\n",
    "Sunny\t Mild\t    Normal\t   Strong\t      Yes\n",
    "Overcast Mild\t    High\t   Strong\t      Yes\n",
    "Overcast Hot\t    Normal\t   Weak\t          Yes\n",
    "Rain\t Mild\t    High\t   Strong\t      No\n",
    "Classify: (Sunny, Cool, High, Strong)\n",
    "Step 1: Calculate Priors\n",
    "\n",
    "P(Yes) = 9/14 ≈ 0.643\n",
    "\n",
    "P(No) = 5/14 ≈ 0.357\n",
    "\n",
    "Step 2: Calculate Likelihoods for \"Yes\"\n",
    "\n",
    "P(Sunny|Yes) = 2/9 ≈ 0.222\n",
    "\n",
    "P(Cool|Yes) = 3/9 ≈ 0.333\n",
    "\n",
    "P(High|Yes) = 3/9 ≈ 0.333\n",
    "\n",
    "P(Strong|Yes) = 3/9 ≈ 0.333\n",
    "\n",
    "Step 3: Calculate Likelihoods for \"No\"\n",
    "\n",
    "P(Sunny|No) = 3/5 = 0.6\n",
    "\n",
    "P(Cool|No) = 1/5 = 0.2\n",
    "\n",
    "P(High|No) = 4/5 = 0.8\n",
    "\n",
    "P(Strong|No) = 3/5 = 0.6\n",
    "\n",
    "Step 4: Calculate Posterior Probabilities\n",
    "\n",
    "P(Yes|X) ∝ 0.643 × 0.222 × 0.333 × 0.333 × 0.333 ≈ 0.0053\n",
    "\n",
    "P(No|X) ∝ 0.357 × 0.6 × 0.2 × 0.8 × 0.6 ≈ 0.0206\n",
    "\n",
    "Step 5: Normalize\n",
    "\n",
    "P(Yes|X) = 0.0053 / (0.0053 + 0.0206) ≈ 0.205\n",
    "\n",
    "P(No|X) = 0.0206 / (0.0053 + 0.0206) ≈ 0.795\n",
    "\n",
    "Prediction: NO (don't play tennis)\n",
    "\n",
    "Step-by-Step Example 2: Text Classification (Spam Detection)\n",
    "Problem: Classify emails as \"Spam\" or \"Not Spam\"\n",
    "\n",
    "Training Data:\n",
    "Email Text\tLabel\n",
    "\"win money now\"\tSpam\n",
    "\"meeting tomorrow\"\tNot Spam\n",
    "\"free lottery\"\tSpam\n",
    "\"project meeting\"\tNot Spam\n",
    "\"buy now win free\"\tSpam\n",
    "\"team lunch\"\tNot Spam\n",
    "Classify: \"free meeting\"\n",
    "Step 1: Calculate Priors\n",
    "\n",
    "P(Spam) = 3/6 = 0.5\n",
    "\n",
    "P(Not Spam) = 3/6 = 0.5\n",
    "\n",
    "Step 2: Build Vocabulary & Calculate Likelihoods\n",
    "Vocabulary: {win, money, now, meeting, free, lottery, project, buy, team, lunch}\n",
    "\n",
    "For Spam:\n",
    "\n",
    "Total words in spam: 8\n",
    "\n",
    "P(\"free\"|Spam) = (1 + 1) / (8 + 10) = 2/18 ≈ 0.111 (with Laplace smoothing)\n",
    "\n",
    "P(\"meeting\"|Spam) = (0 + 1) / (8 + 10) = 1/18 ≈ 0.056\n",
    "\n",
    "For Not Spam:\n",
    "\n",
    "Total words in not spam: 6\n",
    "\n",
    "P(\"free\"|Not Spam) = (0 + 1) / (6 + 10) = 1/16 ≈ 0.0625\n",
    "\n",
    "P(\"meeting\"|Not Spam) = (2 + 1) / (6 + 10) = 3/16 ≈ 0.1875\n",
    "\n",
    "Step 3: Calculate Posterior Probabilities\n",
    "\n",
    "P(Spam|\"free meeting\") ∝ 0.5 × 0.111 × 0.056 ≈ 0.00311\n",
    "\n",
    "P(Not Spam|\"free meeting\") ∝ 0.5 × 0.0625 × 0.1875 ≈ 0.00586\n",
    "\n",
    "Prediction: NOT SPAM\n",
    "\n",
    "Step-by-Step Example 3: Gaussian Naive Bayes (Continuous Data)\n",
    "Problem: Classify flowers based on petal length and width\n",
    "\n",
    "Training Data (simplified):\n",
    "Petal Length\tPetal Width\tSpecies\n",
    "1.4\t0.2\tSetosa\n",
    "1.3\t0.2\tSetosa\n",
    "4.7\t1.4\tVersicolor\n",
    "4.5\t1.5\tVersicolor\n",
    "6.0\t2.5\tVirginica\n",
    "5.9\t1.8\tVirginica\n",
    "Classify: (Petal Length = 5.1, Petal Width = 1.9)\n",
    "Step 1: Calculate Priors\n",
    "\n",
    "P(Setosa) = 2/6 ≈ 0.333\n",
    "\n",
    "P(Versicolor) = 2/6 ≈ 0.333\n",
    "\n",
    "P(Virginica) = 2/6 ≈ 0.333\n",
    "\n",
    "Step 2: Calculate Gaussian Parameters\n",
    "For Versicolor:\n",
    "\n",
    "Petal Length: μ = 4.6, σ = 0.141\n",
    "\n",
    "Petal Width: μ = 1.45, σ = 0.071\n",
    "\n",
    "For Virginica:\n",
    "\n",
    "Petal Length: μ = 5.95, σ = 0.071\n",
    "\n",
    "Petal Width: μ = 2.15, σ = 0.495\n",
    "\n",
    "Step 3: Calculate Gaussian Probabilities\n",
    "Gaussian PDF: P(x|μ,σ) = (1/(σ√(2π))) × exp(-(x-μ)²/(2σ²))\n",
    "\n",
    "For Versicolor:\n",
    "\n",
    "P(Length=5.1|Versicolor) ≈ 0.0002 (using Gaussian calculation)\n",
    "\n",
    "P(Width=1.9|Versicolor) ≈ 0.0001\n",
    "\n",
    "For Virginica:\n",
    "\n",
    "P(Length=5.1|Virginica) ≈ 0.53\n",
    "\n",
    "P(Width=1.9|Virginica) ≈ 0.80\n",
    "\n",
    "Step 4: Calculate Posteriors\n",
    "\n",
    "P(Versicolor|X) ∝ 0.333 × 0.0002 × 0.0001 ≈ 6.67e-9\n",
    "\n",
    "P(Virginica|X) ∝ 0.333 × 0.53 × 0.80 ≈ 0.141\n",
    "\n",
    "Prediction: VIRGINICA\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
